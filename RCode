library(readxl)
library(tidyverse)
library(isdbayes)
library(brms)
library(tidybayes)
dfhc<-read_excel("FHC all data 2018-2023.xlsx", sheet="June")

###General data summaries of length by year
dfhc %>% group_by(Year) %>% summarize(meanTL=mean(Length), SE=sd(Length)/sqrt(length(Length)), 
                                      l25=quantile(Length, probs=0.25), u25=quantile(Length, probs=0.75), 
                                      n=length(Year))


#length-frequency histograms by year
dfhc %>% mutate(Flood = ifelse(Year==2018, "Pre","Post")) %>% 
  ggplot()+
  geom_histogram(aes(x=Length, fill=Flood), binwidth = 10)+
  facet_wrap(~Year, ncol = 1)+
  #scale_x_continuous(breaks=seq(100,1400,by=100))+
  #scale_y_continuous(limits = c(0,25),expand = c(0,0))+
  theme_bw()+
  ylab("Freqency")+
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.title = element_text(size=15), 
        axis.text = element_text(size=12), 
        strip.text = element_text(size=13))+
  scale_fill_manual(values = c("gray10", "gray70"),
                    guide = guide_legend(reverse = TRUE))

##Cumulative length-frequency histograms by year
dfhc %>% mutate(Flood = ifelse(Year==2018, "Pre","Post")) %>% 
  ggplot()+
  stat_ecdf(aes(x=Length, color=Flood, group=Year))+
  #facet_wrap(~Year, ncol = 1)+
  scale_x_continuous(limits=c(0,1300),breaks=seq(0,1400,by=200), expand=c(0,0))+
  scale_y_continuous(expand = c(0,0))+
  theme_bw()+
  ylab("Freqency")+
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        axis.title = element_text(size=15), 
        axis.text = element_text(size=12))+
  scale_color_manual(values = c("black", "gray70"),
                    guide = guide_legend(reverse = TRUE))+
  annotate("label", x = 300, y = 0.5, label = "2018", size=4, col="gray50")+
  annotate("label", x = 250, y = 0.17, label = "2022", size=4)+
  annotate("label", x = 400, y = 0.28, label = "2023", size=4)+
  annotate("label", x = 500, y = 0.34, label = "2024", size=4)
 
## Model Length by Year - start by determining necessary Priors
get_prior(bf(Length ~ Year, shape~Year),
          data = dfhc %>% mutate(Year=as.factor(Year)),
          family = Gamma(link="log"))

#Simulate potential priors using a range of values based on Schall and Lucchesi 2021 expected Flathead sizes
shape.vals=seq(0.5, 10, .5) 
scale.vals=seq(0.5, 6, 0.5) 

#create null object to turn into tibble
tib <- NULL 

#For loop used to run multiple r gamma simulations 
#exponenitate the scale parameter based on brms log link
for (i in 1:20){
  for (j in 1:12){
    vector_name<-paste0("sim",i,j) 
  sim <- rgamma(n=1000, shape=shape.vals[i], scale=exp(scale.vals[j])) 
  tib <- as_tibble(cbind(tib, sim))
  names(tib)[ncol(tib)] <- paste0(i/2,",",j/2)
    }
}

#plot
tib %>% 
  pivot_longer(cols=1:240, names_to = "sim", values_to="values") %>% 
  arrange(sim) %>% 
  group_by(sim) %>% 
  mutate(ID = cur_group_id()) %>% 
  separate_wider_delim(sim, ",", names=c("shape", "scale"))  %>% 
  mutate(shape=as.numeric(shape), 
         scale=as.numeric(scale)) %>%
  ggplot()+
  stat_ecdf(aes(x=values, group=shape, col=shape))+
  facet_wrap(~scale)


#Run Bayesian Model
PrePost_Comp <- brm(bf(Length ~ Year, shape ~ Year),
    data = dfhc %>% mutate(Year=as.factor(Year)),
    family = brmsfamily(family="Gamma", link="log", link_shape="identity"),
    file="Gamma_LFComp_variableShape.rds",
    prior=c(prior(normal(5, 2), class="Intercept"),
            prior(normal(0.5, 0.25), class="b"),
            prior(normal(1, 1), class="b", dpar="shape"),
            prior(normal(5,3), class="Intercept", dpar="shape")),
    cores = 4, chains = 4, iter = 2000)

#Perform model checks
summary(PrePost_Comp)
pp_check(PrePost_Comp, ndraws=40, type="dens_overlay_grouped", group = "Year") + theme_classic()

#ggsave("FigureS3.ppcheck_FHC.jpeg", height=8, width=8, units="in", dpi=800)

#Sample from the posterior
dat_new<-tibble(Year=unique(dfhc$Year))
set.seed(546)
draws <-add_epred_draws(dat_new, PrePost_Comp)
preds <-add_predicted_draws(dat_new, PrePost_Comp)

#Proportion of positive shape beta values
sum(as_draws_df(PrePost_Comp, variable="b_shape_Year2022")$b_shape_Year2022>0)/4000
sum(as_draws_df(PrePost_Comp, variable="b_shape_Year2023")$b_shape_Year2023>0)/4000
sum(as_draws_df(PrePost_Comp, variable="b_shape_Year2024")$b_shape_Year2024>0)/4000

#summarize results of sample and compare to posterior
dfhc %>% group_by(Year) %>% summarize(meanTL=mean(Length), medTL=median(Length))
preds %>% group_by(Year) %>% summarize(meanTL=mean(.prediction), medTL=median(.prediction))
draws %>% group_by(Year) %>% summarize(meanTL=mean(.epred), sd=sd(.epred))

preds %>% group_by(Year) %>% mean_qi(.width=0.5)
#Perform posterior comparison of mean TL values among years
draws_wide<-draws %>%  
  ungroup() %>% 
  select(.draw, Year, .epred) %>% 
  pivot_wider(names_from = Year, values_from=.epred) %>% 
  mutate(mdiff_2022=`2022`-`2018`,
         mdiff_2023=`2023`-`2018`,
         mdiff_2024=`2024`-`2018`,
         mdiff_22_23=`2023`-`2022`, 
         mdiff_22_24=`2024`-`2022`,
         mdiff_23_24=`2024`-`2023`)
sum(draws_wide$mdiff_2022>0)/4000*100
sum(draws_wide$mdiff_2023>0)/4000*100
sum(draws_wide$mdiff_2024>0)/4000*100
sum(draws_wide$mdiff_22_23>0)/4000*100
sum(draws_wide$mdiff_23_24>0)/4000*100

#plot cummulative frequency distributions
preds %>% mutate(Year=as.factor(Year)) %>% 
  ggplot()+
  stat_ecdf(aes(x=.prediction, group=Year, col=Year), linetype=2, linewidth=1)+
  stat_ecdf(data=dfhc,aes(x=Length, col=as.character(Year), group=Year))+
  scale_x_continuous(breaks=seq(0,1600,by=100), limits=c(0,1650))+
  scale_color_manual(values = c("#26045A", "#225ea8", "#41b6c4", "#8EBD9E")) +
  theme_bw()+
  ylab("Freqency")+
  xlab("Total length")+
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        panel.border=element_blank(),
        axis.line.x = element_line(colour = "black"),
        axis.line.y = element_line(colour = "black"),
        axis.title = element_text(size=14), 
        axis.text = element_text(size=10))

#ggsave("Figure3.CummulativeFreq.jpeg", height=5, width=8, units="in", dpi=800)

#Plot posteior overlaps
preds %>% mutate(Year=as.factor(Year)) %>% 
  ggplot(aes(x=.prediction, fill=Year, color=Year))+
  scale_y_continuous(labels=NULL)+
  scale_x_continuous(breaks=seq(0,1600,by=100), limits=c(0,1650))+
  ggridges::geom_density_ridges(aes(y=0),alpha=0.4)+
  scale_fill_manual(values = c("#26045A", "#225ea8", "#41b6c4", "#8EBD9E")) +
  scale_color_manual(values = c("#26045A", "#225ea8", "#41b6c4", "#8EBD9E")) +
  labs(x="Total Length (mm)",
       y="")+
  theme_bw()+
  theme(panel.border = element_blank(), 
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank(),
        axis.line.x = element_line(colour = "black"),
        axis.line.y = element_line(colour = "black"),
        axis.title = element_text(size=14), 
        axis.text = element_text(size=10),
        legend.title=element_blank(),
        axis.ticks.length.y = unit(0, "cm"),
        legend.position="top")

#ggsave("Figure4.PosteriorLFOverlap.jpeg", height=5, width=8, units="in", dpi=800)


#Test for overlap among density distributions
x <- preds %>% filter(Year==2018)
y <- preds %>% filter(Year==2022)

overlapping::overlap(list(x$.prediction, y$.prediction))
